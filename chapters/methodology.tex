\chapter{Methodology}
This chapter presents a general framework for upholding Service Level Objectives (SLOs) in distributed stream processing pipelines deployed in edge computing environments. The methodology addresses the central research question \ref{sec:research-question} of this thesis. To provide an answer, this thesis proposes a unified architecture that combines parallel stream processing with Active Inference (AIF)–based adaptive control. This architecture is designed to elastically adjust system behavior to uphold SLOs in the face of limited computational resources and dynamic workloads. It is designed to be generic, extensible, and applicable across a wide variety of real-time data stream applications at the edge.

\section{System Architecture}
\subsection{Pipeline Design and Parallel Processing}
At the core of our framework lies a modular streaming pipeline composed of three functionally distinct entities: a\textbf{ Producer}, a set of parallel \textbf{Workers}, and a \textbf{
Collector}. The Producer generates a continuous stream of data segments, so-called \textit{tasks}, and enqueues them for processing. A \textit{task} generated by the producer is a simple data structure containing an id, an instruction, and a chunk of data from the underlying stream. The Workers operate in parallel and independently process each task according to the current parameter configurations. Once processed, results are transmitted asynchronously to the Collector, which reassembles and delivers the final output. 

This modular separation enables horizontal scalability and encapsulation of concerns: the Producer governs stream rate and complexity, the Workers handle computation, and the Collector ensures ordering and completion. This design is inspired by the dataflow paradigm typical in modern stream processing systems, such as Apache Storm \cite{carbone_apache_2015, noauthor_apache_nodate}. 

Figure \ref{fig:parallel-distributed-pipeline} provides a high-level overview of the design.

\subsection{Task Distribution and Load Balancing}
Tasks are distributed in a decentralized fashion: each worker node actively requests tasks when ready to compute, forming a pull-based load balancing model. This mechanism enables the system to scale to a large number of workers without requiring global synchronization. This load-balancing pattern emerges naturally as faster or less burdened nodes consume more tasks \cite{estrada_comparing_2015}.

While such an architecture increases communication overhead, as the worker node has to go out of its way to request data, it also enables efficient utilization of heterogeneous compute resources by aligning task assignment with runtime capacity.

\subsection{Asynchronous Communication Mode}
All communication between components is asynchronous. Producers dispatch tasks without blocking \cite{lauener_how_2018}; workers process and return results independently; and collectors operate on incoming data streams without waiting on global completion barriers.

This asynchronous design supports robustness under dynamic conditions, such as fluctuating task loads or varying network bandwidth \cite{nguyen_octopinf_2025}. This non-blocking operation is critical for real-time scenarios in which latency constraints must be maintained regardless of individual node performance fluctuations.

\todo{Bild von der Pipeline einfügen}

\section{Service Level Objectives for Resource Management}
Each SLO defines a desired operational range for a specific performance metric. They are represented as bounded variables whose values must remain within a target range to ensure system stability.

For instance, an SLO limiting memory usage might be formulated as:
\[
\text{Memory} < \theta
\]

Where \(\theta\) denotes the predefined threshold for acceptable memory capacity, e.g., \(\theta = 512\text{MB}\). Violating this constraint signals that the system is exceeding its limitations and requires an elastic adjustment.

In this thesis, SLOs are implemented through a normalized ratio between the current observation $x$ and the associated threshold $\theta$. This ratio, hereafter referred to as the \textit{SLO value}, is computed as

\[
\text{SLO value} = \frac{x}{\theta}
\]

An SLO value less than 1 indicates that the constraint is fulfilled. Conversely, values exceeding 1 
(\(\geq1\)) signal an SLO violation, necessitating corrective action.

For illustration, consider again the memory usage constraint with \(\theta = 512\text{MB}\). A current observation of \(x = 256\text{MB}\) yields an SLO value of \(0.5\), thus satisfying the objective. In contrast, a usage of \(x = 523\text{MB}\) results in

\[
\text{SLO value} = \frac{523}{512} \approx 1.02
\]
indicating a minor SLO violation.

This formulation enables a uniform treatment of heterogeneous resource constraints, allowing the system to monitor, compare, and respond to SLO fulfillment in a principled and quantitative manner.

\subsection{Multi-Dimensional Elasticity}
Each stream processing task can be executed under different configurations, with associated computational demands. A configuration defines a set of \textit{stream quality parameters}, such as inference quality, task frequency and task size. All of these parameters directly impact QoE. The system supports multi-dimensional elasticity, meaning that these parameters can be individually tuned during runtime.

The Producer operates as the central controller of stream parameterization. The changing of stream parameter can be separated into two types:

\begin{itemize}
    \item \textbf{Task generation:} Stream parameters that directly affect the generation of tasks are changed directly at the producer level. The configuration then affects all newly generated tasks.
    \item \textbf{Task processing:} Stream parameters that affect large changes on the worker, such as switching inference quality, require coordination between producer and worker. When the producer dictates a change, it enqueues a control task, detailing the nature of the change, in the backlog of each worker. The next time a worker requests a task from the producer, it receives the control tasks and makes the described changes, before continuing with normal stream processing. This ensures synchronized configuration among workers.
\end{itemize}

\subsection{Continous Monitoring}
It continuously monitors the state of the stream quality parameters and the SLOs. 




The elasticity mechanism enables the adaptive regulation of stream processing configurations in response to the system’s current load and the fulfillment status of predefined SLOs. 



The relationship between these parameters and SLO fulfillment is non-linear. Therefore, the system must learn which configurations fulfill SLOs under which conditions, and which trade-offs are most efficient \cite{sedlak_towards_2025}.

\section{Active Inference-Based Elasticity Control}
\subsection{Generative Model Speicification}
See vague-thesis.pdf

SLOs are encoded as prior preferences. Stream quality parameters are encoded as prior preferences.
\begin{itemize}
  \item \textbf{Observation model \(P(o \mid s)\):} Obeservations of Stream Paramters and SLO values.
  \item \textbf{Transition model \(P(s_{t+1} \mid s_t,a_t)\):} Actions taken by Agent to change stream quality paramters.
  \item \textbf{Prior preferences over observations \(P(o)\):} Agent prefers high stream quality paramters, while strongly averting unfilled SLOs.
  \item \textbf{Prior beliefs about hidden states \(P(s_0)\):} The initial state of SLOs and stream quality parameters, before any data has been processed.
  \item \textbf{Prior beliefs about policies: \(P(\pi)\):} 
\end{itemize}

\subsection{Active Inference Loop Execution}
Loop is executed every 1s.

\section{System Adaptation through Policy Belief Updatin}