\chapter{Methodology}
This chapter presents a general framework for upholding Service Level Objectives (SLOs) in distributed stream processing pipelines deployed in edge computing environments. The methodology addresses the central research question \ref{sec:research-question} of this thesis. To provide an answer, this thesis proposes a unified architecture that combines parallel stream processing with Active Inference (AIF)–based adaptive control. This architecture is designed to elastically adjust system behavior to uphold SLOs in the face of limited computational resources and dynamic workloads. It is designed to be generic, extensible, and applicable across a wide variety of real-time data stream applications at the edge.

\section{System Architecture}
\subsection{Pipeline Design and Parallel Processing}
At the core of our framework lies a modular streaming pipeline composed of three functionally distinct entities: a\textbf{ Producer}, a set of parallel \textbf{Workers}, and a \textbf{
Collector}. The Producer generates a continuous stream of data segments, so-called \textit{tasks}, and enqueues them for processing. A \textit{task} generated by the producer is a simple data structure containing an id, an instruction, and a chunk of data from the underlying stream. The Workers operate in parallel and independently process each task according to the current parameter configurations. Once processed, results are transmitted asynchronously to the Collector, which reassembles and delivers the final output. 

This modular separation enables horizontal scalability and encapsulation of concerns: the Producer governs stream rate and complexity, the Workers handle computation, and the Collector ensures ordering and completion. This design is inspired by the dataflow paradigm typical in modern stream processing systems, such as Apache Storm \cite{carbone_apache_2015, noauthor_apache_nodate}. 

Figure \ref{fig:parallel-distributed-pipeline} provides a high-level overview of the design.

\subsection{Task Distribution and Load Balancing}
Tasks are distributed in a decentralized fashion: each worker node actively requests tasks when ready to compute, forming a pull-based load balancing model. This mechanism enables the system to scale to a large number of workers without requiring global synchronization. This load-balancing pattern emerges naturally as faster or less burdened nodes consume more tasks \cite{estrada_comparing_2015}.

While such an architecture increases communication overhead, as the worker node has to go out of its way to request data, it also enables efficient utilization of heterogeneous compute resources by aligning task assignment with runtime capacity.

\subsection{Asynchronous Communication Model}
All communication between components is asynchronous. Producers dispatch tasks without blocking \cite{lauener_how_2018}; workers process and return results independently; and collectors operate on incoming data streams without waiting on global completion barriers.

This asynchronous design supports robustness under dynamic conditions, such as fluctuating task loads or varying network bandwidth \cite{nguyen_octopinf_2025}. This non-blocking operation is critical for real-time scenarios in which latency constraints must be maintained regardless of individual node performance fluctuations.

\todo{Bild von der Pipeline einfügen}

\section{Service Level Objectives for Resource Management}
Each SLO defines a desired operational range for a specific performance metric. They are represented as bounded variables whose values must remain within a target range to ensure system stability.

For instance, an SLO limiting memory usage might be formulated as:
\[
\text{Memory} < \theta
\]

Where \(\theta\) denotes the predefined threshold for acceptable memory capacity, e.g., \(\theta = 512\text{MB}\). Violating this constraint signals that the system is exceeding its limitations and requires an elastic adjustment.

In this thesis, SLOs are implemented through a normalized ratio between the current observation $x$ and the associated threshold $\theta$. This ratio, hereafter referred to as the \textit{SLO value}, is computed as

\[
\text{SLO value} = \frac{x}{\theta}
\]

An SLO value less than 1 indicates that the constraint is fulfilled. Conversely, values exceeding 1 
(\(\geq1\)) signal an SLO violation, necessitating corrective action.

For illustration, consider again the memory usage constraint with \(\theta = 512\text{MB}\). A current observation of \(x = 256\text{MB}\) yields an SLO value of \(0.5\), thus satisfying the objective. In contrast, a usage of \(x = 523\text{MB}\) results in

\[
\text{SLO value} = \frac{523}{512} \approx 1.02
\]
indicating a minor SLO violation.

This formulation enables a uniform treatment of heterogeneous resource constraints, allowing the system to monitor, compare, and respond to SLO fulfillment in a principled and quantitative manner.

\subsection{Multi-Dimensional Elasticity}
Each stream processing task can be executed under different configurations, with associated computational demands. A configuration defines a set of \textit{stream quality parameters}, such as inference quality, task frequency and task size. All of these parameters directly impact QoE and computational demands. The system supports multi-dimensional elasticity, meaning that these parameters can be individually tuned during runtime.

The Producer operates as the central controller of stream parameterization. The changing of stream parameter can be separated into two types:

\begin{itemize}
    \item \textbf{Task generation:} Stream parameters that directly affect the generation of tasks are changed directly at the producer level. The configuration then affects all newly generated tasks.
    \item \textbf{Task processing:} Stream parameters that affect large changes on the worker, such as switching inference quality, require coordination between producer and worker. When the producer dictates a change, it enqueues a control task, detailing the nature of the change, in the backlog of each worker. The next time a worker requests a task from the producer, it receives the control tasks and makes the described changes, before continuing with normal stream processing. This ensures synchronized configuration among workers.
\end{itemize}
\todo{Bild welches YOLO model change zeigt + Backlog}



\section{Active Inference-Based Elasticity Control}
The system exposes control of the \textit{stream quality parameters} to the AIF agent.
\subsection{Generative Model Construction}
Central to a capable AIF agent is the construction of the \textit{generative model}. It represents the causal relationships between system configuration parameters and observable SLO metrics. It captures both direct (i.e., quality parameters) and indirect dependencies (i.e., SLO) affecting agent behavior. This model includes:
\begin{itemize}
  \item \textbf{Observation model \(P(o \mid s)\):} Obeservations of Stream Paramters and SLO values.
  \item \textbf{Transition model \(P(s_{t+1} \mid s_t,a_t)\):} Actions taken by Agent to change stream quality paramters.
  \item \textbf{Prior preferences over observations \(P(o)\):} Agent prefers high stream quality paramters, while strongly averting unfilled SLOs.
  \item \textbf{Prior beliefs about hidden states \(P(s_0)\):} The initial state of SLOs and stream quality parameters, before any data has been processed.
  \item \textbf{Prior beliefs about policies: \(P(\pi)\):} 
\end{itemize}

\subsubsection{Prioritization Strategy}
Crucially, the SLOs and \textit{stream quality parameters} are encoded as prior preferences with different absolute values. The preference to fulfill an SLO must be stronger than the preference to increase the quality of the stream (i.e., increase computational demand). This results in a lexicographic prioritization, where the primary objective is to maintain SLOs and the secondary objective of maximizing \textit{stream quality parameters}. This prioritization reflects the insight that QoE directly correlates with increased computational cost. Therefore, aggressive maximization of \textit{stream quality parameters} without constraint awareness would lead to SLO violations and system degradation.

The relationship between configuration and SLO fulfillment is non-linear. Therefore, the system must learn which configurations fulfill SLOs under which conditions, and which trade-offs are most efficient \cite{sedlak_towards_2025}.

\subsection{Continous Observation and Configuration Adjustment}
The AIF loop~\ref{sec:active-inference-loop} is executed periodically during the producer's runtime to evaluate and adapt the system's configuration. Finding an appropriate interval between evaluations is critical, as it determines how frequently the agent can revise and apply its control policy. Short intervals (e.g., 50\,ms) allow rapid responsiveness but risk instability. Actions may be issued faster than their causal effects can propagate through the system, leading to overcompensation or oscillatory behavior. This is especially problematic in stream processing pipelines where SLO-relevant variables such as memory usage evolve with delay. Conversely, overly long intervals (e.g., 20\,s) hinder the system’s ability to respond to rapid environmental changes, potentially causing SLO violations due to latency in corrective actions. 


\subsubsection{Active Inference Loop}
Each iteration of the loop involves a three-step procedure: observation, state and policy inference, and action sampling.

\todo{refactor this using pymdp paper}
\begin{enumerate}
    \item \textbf{Observation (Perception):} The Agent receives updated observations of the current stream quality parameters and normalized SLO values. These observations form the input to the generative model and encode the current operational state of the distributed system.
    \item \textbf{State and Policy inference (Belief Updating):} The agent uses the observations to infer hidden states that represent the compute budget. Inference is performed using the generative model, which captures causal dependencies between actions, configurations, and SLO outcomes. 
    
    Each policy proposes a sequence of future actions (e.g., lowering frame rate, switching inference models) over a short receding horizon defined by \(\pi\). These are scored against the agent’s preference model, which penalizes predicted SLO violations and rewards higher quality configurations—subject to constraints. Policies are ranked by their expected utility, defined via expected outcomes under the current posterior. Policies that minimize anticipated SLO violations while improving QoE are preferred.

    Policies are ranked by their expected utility, defined via expected outcomes under the current posterior. Policies that minimize anticipated SLO violations while improving QoE are preferred.
    
    \item \textbf{Choosing Actions:} The highest-ranked policy is selected, and its first action is executed. This action corresponds to a reconfiguration command issued to the Producer. Depending on the type of change, the action either alters the global task generation template or injects a control message into the worker backlog (see Section~\ref{sec:stream-param-coordination}).

The updated configuration becomes active immediately, and the system continues streaming.
\end{enumerate}




\subsection{Homeostasis}
The agent's long-term objective is the persistent minimization of \textit{surprise} \cite{sedlak_adaptive_2024}, achieving homeostasis. This is done by finding a configuration that maximizes QoE, while upholding all SLOs.

\subsection{Continous Monitoring}

TODO: Implement these factors in "Active Inference-Based Elasticity Control"
It continuously monitors the state of the stream quality parameters and the SLOs. 

The elasticity mechanism enables the adaptive regulation of stream processing configurations in response to the system’s current load and the fulfillment status of predefined SLOs. 



\section{System Adaptation through Policy Belief Updating}