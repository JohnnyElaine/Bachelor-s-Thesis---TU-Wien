\chapter{Introduction}

The rapid growth of Internet of Things (IoT) devices, mobile computing applications, and real-time sensor networks has led to an exponential increase in data streams generated at the edge of the network. As a consequence, edge computing has emerged as a distributed paradigm, wherein processing and decision-making are performed as close as possible to data sources~\cite{deng_edge_2020}. The core promise of edge computing lies in its capacity to
reduce latency, enhance privacy, save bandwidth, and improve responsiveness by processing data closer to the sources of generation~\cite{deng_edge_2020}. 

However, edge computing introduces its own set of challenges. Edge devices are highly heterogeneous~\cite{furst_elastic_2018}, constrained in terms of compute, memory, energy availability, and operate in unpredictable environments~\cite{sedlak_active_2024, danilenka_adaptive_2025}. This complexity is exacerbated by Quality of Service (QoS) and Quality of Experience (QoE) standards that modern edge computing scenarios must sustain. The ability to provide high-quality service under such conditions is critically dependent on the capacity to monitor, predict, and enforce Service Level Objectives (SLOs)---quantitative targets, such as latency, throughput, and resource utilization~\cite{sedlak_diffusing_2024, nastic_sloc_2020}. 

Conventional elasticity mechanisms and autoscalers typically address SLO compliance by scaling computational resources. However, such resource scaling strategies are ineffective in edge computing, where additional capacity cannot be provisioned dynamically. Instead, the focus on elasticity must shift toward the services themselves. Inspired by Sedlak et al.~\cite{sedlak_towards_2025}, this thesis adopts a multi-dimensional elasticity approach that adapts the quality and processing configuration of the stream data, rather than modifying the underlying infrastructure.

Ensuring SLO compliance in these environments remains an unsolved problem. The necessity of elasticity---that is, dynamically scaling application parameters in response to workload fluctuations and SLO states---demands new control paradigms that are both data-driven and interpretable~\cite{lapkovskis_benchmarking_2025, dias_de_assuncao_distributed_2018}.

Traditional solutions, such as threshold-based scaling or Reinforcement Learning (RL), often fail to provide robust guarantees or require costly retraining under shifting environmental conditions~\cite{xu_coscal_2022}. Moreover, they often do not possess adequate predictive capabilities, making real-time adaptation difficult under rapidly changing workloads and operational conditions~\cite{oquinn_environment-aware_2025}.

In response to these limitations, advanced approaches that incorporate predictive and adaptive
capabilities are necessary. Active Inference (AIF), a concept derived from neuroscience, as introduced by Karl J. Friston in~\cite{friston_free-energy_2010}, has recently emerged as a methodology to address such challenges. AIF provides a unified mathematical framework enabling agents to
continually anticipate and adapt to uncertainty through perception-action cycle~\cite{friston_active_2016, lanillos_active_2021}. This thesis addresses the challenge of integrating AIF principles in a distributed system environment to adapt resource allocation and maintain QoS and QoE under strict constraints.

\section{Challenges and Problem Definition}
Designing adaptive edge systems for real-time data streams presents several key
challenges. First, edge environments are highly heterogeneous~\cite{furst_elastic_2018}, with varying capabilities and
rapidly shifting workloads~\cite{danilenka_adaptive_2025}. Especially as Artificial Intelligence (AI) expands into the edge, it creates a heightened demand for high-performance, low-latency inference across devices~\cite{oquinn_environment-aware_2025}. Second, unlike in centralized settings, edge systems must make do with varying resource pools~\cite{sedlak_equilibrium_2024}.

Traditional distributed computing frameworks, such as MapReduce~\cite{dean_mapreduce_2008},
Apache Storm\cite{noauthor_apache_nodate} and Apache Flink~\cite{noauthor_apache_nodate-1, carbone_apache_2015}, have significantly advanced the parallelization and
distribution of data-intensive computations. Nevertheless, these frameworks were initially
developed for environments with relatively abundant resources and predictable workloads,
making their direct application to highly dynamic edge scenarios challenging. Conventional elasticity management practices, which often involve heuristic-based rules or reactive scaling methods, are insufficient to address the adaptation requirements of edge computing environments. Sedlak et al. mention in~\cite{sedlak_diffusing_2024} the fact that there is still a significant lack of integrated frameworks that combine predictive modeling, real-time inference, dynamic elasticity, and strict SLO enforcement. As a result, there is a strong need for comprehensive methods that use advanced prediction to proactively manage resources and maintain performance in the constrained conditions of edge computing.

This relates to the third major challenge \textit{continuous adaptation}\cite{danilenka_adaptive_2025}. A popular alternative to threshold-based agents are Machine Learning (ML) based approaches to adapt the system. However, as Sedlak et al. observe in~\cite{sedlak_active_2024}, these approaches fail to capture gradual changes in the environment, as long as the ML models are not retrained, leading to an inaccurate view of the system. This creates the need for an agent that continuously adapts to its environment~\cite{sedlak_equilibrium_2024}.


\section{Research Questions and Objectives}
\label{sec:research-question}

Given the outlined challenges and research gaps, the primary objective of this thesis is to investigate how Active Inference can be used to uphold Service Level Objectives (SLOs) in resource-constrained edge computing environments. The following research questions guide the work:

\begin{enumerate}
    \item \textbf{Efficient Stream Processing:} 
    \begin{quote}
        How can streaming data be processed efficiently in edge computing systems with heterogeneous computational resources?
    \end{quote}
    
    \item \textbf{Elasticity Mechanisms:}
    \begin{quote}
        How can elasticity be implemented by adapting stream service parameters to manage computational load?
    \end{quote}
    
    \item \textbf{Control via Active Inference:}
    \begin{quote}
        How can an Active Inference agent be used to select stream configurations that balance SLO enforcement and output quality?
    \end{quote}
    
    \item \textbf{Trade-off Analysis:}
    \begin{quote}
        What are the trade-offs between upholding SLOs and maximizing Quality of Experience (QoE) in dynamic and resource-limited scenarios?
    \end{quote}
\end{enumerate}

To address the research questions, this thesis pursues the following objectives:

\begin{itemize}
    \item Design and implement a distributed, parallel pipeline for stream processing on resource-constrained edge devices.
    
    \item Define and enforce Service Level Objectives (SLOs) as first-class constraints to regulate resource usage and latency across the system.
    
    \item Introduce an elasticity mechanism that dynamically adapts core stream parameters in response to varying workload and resource availability.
    
    \item Integrate an Active Inference (AIF) agent to govern elasticity decisions in a principled manner, enabling continuous satisfaction of SLOs under uncertainty.
    
    \item Evaluate the effectiveness of the proposed approach using a prototype implementation for real-time video processing with YOLO-based object detection, focusing on trade-offs between output quality and SLO compliance.
\end{itemize}


\section{Proposed Approach}
This thesis proposes a novel distributed parallel pipeline architecture for edge data stream processing, which is tightly coupled with a real-time elasticity mechanism governed by an active inference agent. In this architecture, data streams are partitioned and processed in parallel across multiple worker nodes. Each worker executes inference or transformation tasks, while a collector node aggregates the results. Crucially, a central agent continuously monitors SLOs and system state, and adjusts stream parameters to maximize Quality of Experience (QoE). To balance the resource- and QoE requirements, the system implements Service Level Objectives (SLOs) to meet the performance and quality targets of the system~\cite{sedlak_towards_2025, nastic_sloc_2020}.

The distinguishing contribution of this work is the application of active inference as a generic,
explainable control paradigm for adaptive elasticity. By framing SLO satisfaction and quality maximization as explicit preferences within a generative model, the agent is able to select actions that balance exploitation (maximizing stream quality) and exploration (reducing model uncertainty), in accordance with current observations and beliefs~\cite{casamayor_pujol_deepslos_2024, sedlak_adaptive_2024, danilenka_adaptive_2025, lanillos_active_2021}. This approach is generalizable to arbitrary data streams (including audio, video, sensor, or network data), and can support a range of SLO types and system constraints.

