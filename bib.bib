
@misc{oquinn_environment-aware_2025,
	title = {Environment-{Aware} {Dynamic} {Pruning} for {Pipelined} {Edge} {Inference}},
	url = {http://arxiv.org/abs/2503.03070},
	doi = {10.48550/arXiv.2503.03070},
	abstract = {IoT and edge-based inference systems require unique solutions to overcome resource limitations and unpredictable environments. In this paper, we propose an environment-aware dynamic pruning system that handles the unpredictability of edge inference pipelines. While traditional pruning approaches can reduce model footprint and compute requirements, they are often performed only once, offline, and are not designed to react to transient or post-deployment device conditions. Similarly, existing pipeline placement strategies may incur high overhead if reconfigured at runtime, limiting their responsiveness. Our approach, allows slices of a model—already placed on a distributed pipeline—to be ad-hoc pruned as a means of load-balancing. To support this capability, we introduce two key components: (1) novel training strategies that endow models with robustness to post-deployment pruning, and (2) an adaptive algorithm that determines the optimal pruning level for each node based on monitored bottlenecks. In real-world experiments, on a Raspberry Pi 4B cluster running cameratrap workloads, our method achieves a 1.5× speedup and a 3× improvement in service-level objective (SLO) attainment, all while maintaining high accuracy.},
	language = {en},
	urldate = {2025-05-14},
	publisher = {arXiv},
	author = {O'Quinn, Austin and Snedeker, Conor and Zhang, Siyuan and Kline, Jenna},
	month = mar,
	year = {2025},
	note = {arXiv:2503.03070 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {PDF:C\:\\Users\\elias\\Zotero\\storage\\YZULLNIQ\\O'Quinn et al. - 2025 - Environment-Aware Dynamic Pruning for Pipelined Edge Inference.pdf:application/pdf;Snapshot:C\:\\Users\\elias\\Zotero\\storage\\UK5RPX9P\\2503.html:text/html},
}

@misc{nguyen_octopinf_2025,
	title = {{OCTOPINF}: {Workload}-{Aware} {Inference} {Serving} for {Edge} {Video} {Analytics}},
	shorttitle = {{OCTOPINF}},
	url = {http://arxiv.org/abs/2502.01277},
	doi = {10.48550/arXiv.2502.01277},
	abstract = {Edge Video Analytics (EVA) has gained significant attention as a major application of pervasive computing, enabling real-time visual processing. EVA pipelines, composed of deep neural networks (DNNs), typically demand efficient inference serving under stringent latency requirements, which is challenging due to the dynamic Edge environments (e.g., workload variability and network instability). Moreover, EVA pipelines also face significant resource contention caused by resource (e.g., GPU) constraints at the Edge. In this paper, we introduce OCTOPINF, a novel resource-efficient and workload-aware inference serving system designed for real-time EVA. OCTOPINF tackles the unique challenges of dynamic edge environments through fine-grained resource allocation, adaptive batching, and workload balancing between edge devices and servers. Furthermore, we propose a spatiotemporal scheduling algorithm that optimizes the co-location of inference tasks on GPUs, improving performance and ensuring service-level objectives (SLOs) compliance. Extensive evaluations on a real-world testbed demonstrate the effectiveness of our approach. It achieves an effective throughput increase of up to 10x compared to the baselines and shows better robustness in challenging scenarios. OCTOPINF can be used for any DNN-based EVA inference task with minimal adaptation and is available at https://github.com/tungngreen/PipelineScheduler.},
	urldate = {2025-05-14},
	publisher = {arXiv},
	author = {Nguyen, Thanh-Tung and Liebe, Lucas and Tau, Nhat-Quang and Wu, Yuheng and Cheng, Jinghan and Lee, Dongman},
	month = feb,
	year = {2025},
	note = {arXiv:2502.01277 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {Preprint PDF:C\:\\Users\\elias\\Zotero\\storage\\ZAYZY5R3\\Nguyen et al. - 2025 - OCTOPINF Workload-Aware Inference Serving for Edge Video Analytics.pdf:application/pdf;Snapshot:C\:\\Users\\elias\\Zotero\\storage\\DUZ99XMD\\2502.html:text/html},
}

@inproceedings{lin_murmuration_2024,
	address = {New York, NY, USA},
	series = {{ICPP} '24},
	title = {Murmuration: {On}-the-fly {DNN} {Adaptation} for {SLO}-{Aware} {Distributed} {Inference} in {Dynamic} {Edge} {Environments}},
	isbn = {979-8-4007-1793-2},
	shorttitle = {Murmuration},
	url = {https://dl.acm.org/doi/10.1145/3673038.3673154},
	doi = {10.1145/3673038.3673154},
	abstract = {The proliferation of Virtual and Augmented Reality (VR/AR) and the Internet of Things (IoT) applications is driving the demand for efficient Deep Neural Network (DNN) inference at the edge. These applications often impose stringent Service Level Objectives (SLOs), such as latency or accuracy, that must be met under the constraints of limited resources and dynamic network conditions. In this study, we explore a novel approach to DNN inference across multiple edge devices, incorporating both model customization and partitioning dynamically, to better align with these constraints and SLOs. Unlike conventional methods that employ a single fixed DNN network, our system, termed\&nbsp;Murmuration, combines one-shot Neural Architecture Search (NAS) and Reinforcement Learning (RL) to dynamically customize and partition DNN models. This approach adapts in real-time to the capabilities of the edge devices, network conditions, and varying SLO requirements. The design of Murmuration allows it to effectively navigate the large search space defined by DNN models, network delays, and bandwidth, offering a significant improvement in managing trade-offs between accuracy and latency. We implemented and evaluated Murmuration using a variety of edge devices. The results show that our approach outperforms state-of-the-art methods in terms of inference accuracy by up to \&lt;Formula format="inline"\&gt;\&lt;TexMath\&gt;\&lt;?TeX \$5{\textbackslash}\%\$?\&gt;\&lt;/TexMath\&gt;\&lt;AltText\&gt;Math 1\&lt;/AltText\&gt;\&lt;File name="icpp24-116-inline1" type="svg"/\&gt;\&lt;/Formula\&gt; or latency by up to 6.7 ×. With the flexibility of model customization, Murmuration can meet SLO under a wider range of network delays and bandwidths, improving SLO compliance rate by up to 52\%.},
	urldate = {2025-05-14},
	booktitle = {Proceedings of the 53rd {International} {Conference} on {Parallel} {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Lin, Jieyu and Li, Minghao and Zhang, Sai Qian and Leon-Garcia, Alberto},
	month = aug,
	year = {2024},
	pages = {792--801},
	file = {Full Text PDF:C\:\\Users\\elias\\Zotero\\storage\\V2MNUV9D\\Lin et al. - 2024 - Murmuration On-the-fly DNN Adaptation for SLO-Aware Distributed Inference in Dynamic Edge Environme.pdf:application/pdf},
}

@inproceedings{horovitz_efficient_2018,
	title = {Efficient {Cloud} {Auto}-{Scaling} with {SLA} {Objective} {Using} {Q}-{Learning}},
	url = {https://ieeexplore.ieee.org/document/8457997},
	doi = {10.1109/FiCloud.2018.00020},
	abstract = {Threshold based cloud auto-scaling is one of the most common methods used to scale cloud applications. A major drawback of this method is that the thresholds are set manually by the user in an ad hoc fashion, not optimally, and specially crafted for a specific application behavior, leading to SLA failures. We present Q-Threshold - A novel algorithm for adaptively and dynamically adjusting the thresholds with no need for user configuration while meeting SLA objectives. In this context we present new methods for improving reinforcement Q-Learning auto-scaling with faster convergence, reduced state space and reduced action space in a distributed cloud environment. We demonstrate the effectiveness of our methods both on simulations and on real applications.},
	urldate = {2025-05-14},
	booktitle = {2018 {IEEE} 6th {International} {Conference} on {Future} {Internet} of {Things} and {Cloud} ({FiCloud})},
	author = {Horovitz, Shay and Arian, Yair},
	month = aug,
	year = {2018},
	keywords = {Adaptation models, Aerospace electronics, Auto Scaling, Cloud, Cloud computing, Convergence, Explosions, Learning (artificial intelligence), Machine Learning, Q-Learning, SLA, Time factors},
	pages = {85--92},
	file = {Full Text PDF:C\:\\Users\\elias\\Zotero\\storage\\8ZHS3ARX\\Horovitz and Arian - 2018 - Efficient Cloud Auto-Scaling with SLA Objective Using Q-Learning.pdf:application/pdf},
}

@article{xu_coscal_2022,
	title = {{CoScal}: {Multifaceted} {Scaling} of {Microservices} {With} {Reinforcement} {Learning}},
	volume = {19},
	issn = {1932-4537},
	shorttitle = {{CoScal}},
	url = {https://ieeexplore.ieee.org/abstract/document/9904920},
	doi = {10.1109/TNSM.2022.3210211},
	abstract = {The emerging trend towards moving from monolithic applications to microservices has raised new performance challenges in cloud computing environments. Compared with traditional monolithic applications, the microservices are lightweight, fine-grained, and must be executed in a shorter time. Efficient scaling approaches are required to ensure microservices’ system performance under diverse workloads with strict Quality of Service (QoS) requirements and optimize resource provisioning. To solve this problem, we investigate the trade-offs between the dominant scaling techniques, including horizontal scaling, vertical scaling, and brownout in terms of execution cost and response time. We first present a prediction algorithm based on gradient recurrent units to accurately predict workloads assisting in scaling to achieve efficient scaling. Further, we propose a multi-faceted scaling approach using reinforcement learning called CoScal to learn the scaling techniques efficiently. The proposed CoScal approach takes full advantage of data-driven decisions and improves the system performance in terms of high communication cost and delay. We validate our proposed solution by implementing a containerized microservice prototype system and evaluated with two microservice applications. The extensive experiments demonstrate that CoScal reduces response time by 19\%-29\% and decreases the connection time of services by 16\% when compared with the state-of-the-art scaling techniques for Sock Shop application. CoScal can also improve the number of successful transactions with 6\%-10\% for Stan’s Robot Shop application.},
	number = {4},
	urldate = {2025-05-14},
	journal = {IEEE Transactions on Network and Service Management},
	author = {Xu, Minxian and Song, Chenghao and Ilager, Shashikant and Gill, Sukhpal Singh and Zhao, Juanjuan and Ye, Kejiang and Xu, Chengzhong},
	month = dec,
	year = {2022},
	keywords = {Cloud computing, brownout, Costs, Heuristic algorithms, Microservice architectures, microservices, Predictive models, Quality of service, reinforcement learning, Reinforcement learning, scalability, workload prediction},
	pages = {3995--4009},
	file = {Full Text PDF:C\:\\Users\\elias\\Zotero\\storage\\CNGKQYJR\\Xu et al. - 2022 - CoScal Multifaceted Scaling of Microservices With Reinforcement Learning.pdf:application/pdf},
}

@inproceedings{zhang_octopus_2023,
	address = {Cham},
	title = {Octopus: {SLO}-{Aware} {Progressive} {Inference} {Serving} via {Deep} {Reinforcement} {Learning} in {Multi}-tenant {Edge} {Cluster}},
	isbn = {978-3-031-48424-7},
	shorttitle = {Octopus},
	doi = {10.1007/978-3-031-48424-7_18},
	abstract = {Deep neural network (DNN) inference service at the edge is promising, but it is still non-trivial to achieve high-throughput for multi-DNN model deployment on resource-constrained edge devices. Furthermore, an edge inference service system must respond to requests with bounded latency to maintain a consistent service-level objective (SLO). To address these challenges, we propose Octopus, a flexible and adaptive SLO-aware progressive inference scheduling framework to support both computer vision (CV) and natural language processing (NLP) DNN models on a multi-tenant heterogeneous edge cluster. Our deep reinforcement learning-based scheduler can automatically determine the optimal joint configuration of 1) DNN batch size, 2) DNN model exit point, and 3) edge node dispatching for each inference request to maximize the overall throughput of edge clusters. We evaluate Octopus using representative CV and NLP DNN models on an edge cluster with various heterogeneous devices. Our extensive experiments reveal that Octopus is adaptive to various requests and dynamic networks, achieving up to a 3.3\$\${\textbackslash}times \$\$×improvement in overall throughput compared to state-of-the-art schemes while satisfying soft SLO and maintaining high inference accuracy.},
	language = {en},
	booktitle = {Service-{Oriented} {Computing}},
	publisher = {Springer Nature Switzerland},
	author = {Zhang, Ziyang and Zhao, Yang and Liu, Jie},
	editor = {Monti, Flavia and Rinderle-Ma, Stefanie and Ruiz Cortés, Antonio and Zheng, Zibin and Mecella, Massimo},
	year = {2023},
	keywords = {Deep reinforcement learning, Edge computing, Multi-tenant, Progressive inference},
	pages = {242--258},
	file = {Full Text PDF:C\:\\Users\\elias\\Zotero\\storage\\BW86MYC7\\Zhang et al. - 2023 - Octopus SLO-Aware Progressive Inference Serving via Deep Reinforcement Learning in Multi-tenant Edg.pdf:application/pdf},
}

@article{friston_active_2017,
	title = {Active {Inference}: {A} {Process} {Theory}},
	volume = {29},
	issn = {0899-7667},
	shorttitle = {Active {Inference}},
	url = {https://doi.org/10.1162/NECO_a_00912},
	doi = {10.1162/NECO_a_00912},
	abstract = {This article describes a process theory based on active inference and belief propagation. Starting from the premise that all neuronal processing (and action selection) can be explained by maximizing Bayesian model evidence—or minimizing variational free energy—we ask whether neuronal responses can be described as a gradient descent on variational free energy. Using a standard (Markov decision process) generative model, we derive the neuronal dynamics implicit in this description and reproduce a remarkable range of well-characterized neuronal phenomena. These include repetition suppression, mismatch negativity, violation responses, place-cell activity, phase precession, theta sequences, theta-gamma coupling, evidence accumulation, race-to-bound dynamics, and transfer of dopamine responses. Furthermore, the (approximately Bayes’ optimal) behavior prescribed by these dynamics has a degree of face validity, providing a formal explanation for reward seeking, context learning, and epistemic foraging. Technically, the fact that a gradient descent appears to be a valid description of neuronal activity means that variational free energy is a Lyapunov function for neuronal dynamics, which therefore conform to Hamilton’s principle of least action.},
	number = {1},
	urldate = {2025-05-14},
	journal = {Neural Computation},
	author = {Friston, Karl and FitzGerald, Thomas and Rigoli, Francesco and Schwartenbeck, Philipp and Pezzulo, Giovanni},
	month = jan,
	year = {2017},
	pages = {1--49},
	file = {Accepted Version:C\:\\Users\\elias\\Zotero\\storage\\N7ZKZZL7\\Friston et al. - 2017 - Active Inference A Process Theory.pdf:application/pdf;Snapshot:C\:\\Users\\elias\\Zotero\\storage\\85QUY29A\\Active-Inference-A-Process-Theory.html:text/html},
}

@article{friston_active_2016,
	title = {Active inference and learning},
	volume = {68},
	issn = {0149-7634},
	url = {https://www.sciencedirect.com/science/article/pii/S0149763416301336},
	doi = {10.1016/j.neubiorev.2016.06.022},
	abstract = {This paper offers an active inference account of choice behaviour and learning. It focuses on the distinction between goal-directed and habitual behaviour and how they contextualise each other. We show that habits emerge naturally (and autodidactically) from sequential policy optimisation when agents are equipped with state-action policies. In active inference, behaviour has explorative (epistemic) and exploitative (pragmatic) aspects that are sensitive to ambiguity and risk respectively, where epistemic (ambiguity-resolving) behaviour enables pragmatic (reward-seeking) behaviour and the subsequent emergence of habits. Although goal-directed and habitual policies are usually associated with model-based and model-free schemes, we find the more important distinction is between belief-free and belief-based schemes. The underlying (variational) belief updating provides a comprehensive (if metaphorical) process theory for several phenomena, including the transfer of dopamine responses, reversal learning, habit formation and devaluation. Finally, we show that active inference reduces to a classical (Bellman) scheme, in the absence of ambiguity.},
	urldate = {2025-05-14},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Friston, Karl and FitzGerald, Thomas and Rigoli, Francesco and Schwartenbeck, Philipp and O⿿Doherty, John and Pezzulo, Giovanni},
	month = sep,
	year = {2016},
	keywords = {Active inference, Bayesian inference, Bayesian surprise, Epistemic value, Exploitation, Exploration, Free energy, Goal-directed, Habit learning, Information gain},
	pages = {862--879},
	file = {Full Text:C\:\\Users\\elias\\Zotero\\storage\\G28UZ648\\Friston et al. - 2016 - Active inference and learning.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\elias\\Zotero\\storage\\QVF6MNGJ\\S0149763416301336.html:text/html},
}

@article{friston_free-energy_2010,
	title = {The free-energy principle: a unified brain theory?},
	volume = {11},
	copyright = {2010 Springer Nature Limited},
	issn = {1471-0048},
	shorttitle = {The free-energy principle},
	url = {https://www.nature.com/articles/nrn2787},
	doi = {10.1038/nrn2787},
	abstract = {Adaptive agents must occupy a limited repertoire of states and therefore minimize the long-term average of surprise associated with sensory exchanges with the world. Minimizing surprise enables them to resist a natural tendency to disorder.Surprise rests on predictions about sensations, which depend on an internal generative model of the world. Although surprise cannot be measured directly, a free-energy bound on surprise can be, suggesting that agents minimize free energy by changing their predictions (perception) or by changing the predicted sensory inputs (action).Perception optimizes predictions by minimizing free energy with respect to synaptic activity (perceptual inference), efficacy (learning and memory) and gain (attention and salience). This furnishes Bayes-optimal (probabilistic) representations of what caused sensations (providing a link to the Bayesian brain hypothesis).Bayes-optimal perception is mathematically equivalent to predictive coding and maximizing the mutual information between sensations and the representations of their causes. This is a probabilistic generalization of the principle of efficient coding (the infomax principle) or the minimum-redundancy principle.Learning under the free-energy principle can be formulated in terms of optimizing the connection strengths in hierarchical models of the sensorium. This rests on associative plasticity to encode causal regularities and appeals to the same synaptic mechanisms as those underlying cell assembly formation.Action under the free-energy principle reduces to suppressing sensory prediction errors that depend on predicted (expected or desired) movement trajectories. This provides a simple account of motor control, in which action is enslaved by perceptual (proprioceptive) predictions.Perceptual predictions rest on prior expectations about the trajectory or movement through the agent's state space. These priors can be acquired (as empirical priors during hierarchical inference) or they can be innate (epigenetic) and therefore subject to selective pressure.Predicted motion or state transitions realized by action correspond to policies in optimal control theory and reinforcement learning. In this context, value is inversely proportional to surprise (and implicitly free energy), and rewards correspond to innate priors that constrain policies.},
	language = {en},
	number = {2},
	urldate = {2025-05-14},
	journal = {Nature Reviews Neuroscience},
	author = {Friston, Karl},
	month = feb,
	year = {2010},
	note = {Publisher: Nature Publishing Group},
	keywords = {Control theory, Neural encoding},
	pages = {127--138},
	file = {PDF:C\:\\Users\\elias\\Zotero\\storage\\2MN5SS6F\\Friston - 2010 - The free-energy principle a unified brain theory.pdf:application/pdf},
}

@article{smith_step-by-step_2022,
	title = {A step-by-step tutorial on active inference and its application to empirical data},
	volume = {107},
	issn = {0022-2496},
	url = {https://www.sciencedirect.com/science/article/pii/S0022249621000973},
	doi = {10.1016/j.jmp.2021.102632},
	abstract = {The active inference framework, and in particular its recent formulation as a partially observable Markov decision process (POMDP), has gained increasing popularity in recent years as a useful approach for modeling neurocognitive processes. This framework is highly general and flexible in its ability to be customized to model any cognitive process, as well as simulate predicted neuronal responses based on its accompanying neural process theory. It also affords both simulation experiments for proof of principle and behavioral modeling for empirical studies. However, there are limited resources that explain how to build and run these models in practice, which limits their widespread use. Most introductions assume a technical background in programming, mathematics, and machine learning. In this paper we offer a step-by-step tutorial on how to build POMDPs, run simulations using standard MATLAB routines, and fit these models to empirical data. We assume a minimal background in programming and mathematics, thoroughly explain all equations, and provide exemplar scripts that can be customized for both theoretical and empirical studies. Our goal is to provide the reader with the requisite background knowledge and practical tools to apply active inference to their own research. We also provide optional technical sections and multiple appendices, which offer the interested reader additional technical details. This tutorial should provide the reader with all the tools necessary to use these models and to follow emerging advances in active inference research.},
	urldate = {2025-05-14},
	journal = {Journal of Mathematical Psychology},
	author = {Smith, Ryan and Friston, Karl J. and Whyte, Christopher J.},
	month = apr,
	year = {2022},
	keywords = {Active inference, Bayesian inference, Computational neuroscience, Decision-making, Learning, Machine learning},
	pages = {102632},
	file = {Full Text:C\:\\Users\\elias\\Zotero\\storage\\7G7JJ4QM\\Smith et al. - 2022 - A step-by-step tutorial on active inference and its application to empirical data.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\elias\\Zotero\\storage\\L8JVC4I4\\S0022249621000973.html:text/html},
}

@article{friston_free-energy_2009,
	title = {The free-energy principle: a rough guide to the brain?},
	volume = {13},
	issn = {1364-6613, 1879-307X},
	shorttitle = {The free-energy principle},
	url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(09)00117-X},
	doi = {10.1016/j.tics.2009.04.005},
	language = {English},
	number = {7},
	urldate = {2025-05-14},
	journal = {Trends in Cognitive Sciences},
	author = {Friston, Karl},
	month = jul,
	year = {2009},
	pmid = {19559644},
	note = {Publisher: Elsevier},
	pages = {293--301},
	file = {Full Text PDF:C\:\\Users\\elias\\Zotero\\storage\\NWY8ITA7\\Friston - 2009 - The free-energy principle a rough guide to the brain.pdf:application/pdf},
}

@article{heins_pymdp_2022,
	title = {pymdp: {A} {Python} library for active inference in discrete state spaces},
	volume = {7},
	issn = {2475-9066},
	shorttitle = {pymdp},
	url = {http://arxiv.org/abs/2201.03904},
	doi = {10.21105/joss.04098},
	abstract = {Active inference is an account of cognition and behavior in complex systems which brings together action, perception, and learning under the theoretical mantle of Bayesian inference. Active inference has seen growing applications in academic research, especially in fields that seek to model human or animal behavior. While in recent years, some of the code arising from the active inference literature has been written in open source languages like Python and Julia, to-date, the most popular software for simulating active inference agents is the DEM toolbox of SPM, a MATLAB library originally developed for the statistical analysis and modelling of neuroimaging data. Increasing interest in active inference, manifested both in terms of sheer number as well as diversifying applications across scientific disciplines, has thus created a need for generic, widely-available, and user-friendly code for simulating active inference in open-source scientific computing languages like Python. The Python package we present here, pymdp (see https://github.com/infer-actively/pymdp), represents a significant step in this direction: namely, we provide the first open-source package for simulating active inference with partially-observable Markov Decision Processes or POMDPs. We review the package's structure and explain its advantages like modular design and customizability, while providing in-text code blocks along the way to demonstrate how it can be used to build and run active inference processes with ease. We developed pymdp to increase the accessibility and exposure of the active inference framework to researchers, engineers, and developers with diverse disciplinary backgrounds. In the spirit of open-source software, we also hope that it spurs new innovation, development, and collaboration in the growing active inference community.},
	number = {73},
	urldate = {2025-05-14},
	journal = {Journal of Open Source Software},
	author = {Heins, Conor and Millidge, Beren and Demekas, Daphne and Klein, Brennan and Friston, Karl and Couzin, Iain and Tschantz, Alexander},
	month = may,
	year = {2022},
	note = {arXiv:2201.03904 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Mathematical Software, Quantitative Biology - Neurons and Cognition},
	pages = {4098},
	file = {Preprint PDF:C\:\\Users\\elias\\Zotero\\storage\\T9BC85BE\\Heins et al. - 2022 - pymdp A Python library for active inference in discrete state spaces.pdf:application/pdf;Snapshot:C\:\\Users\\elias\\Zotero\\storage\\NUGURXIA\\2201.html:text/html},
}

@misc{lanillos_active_2021,
	title = {Active {Inference} in {Robotics} and {Artificial} {Agents}: {Survey} and {Challenges}},
	shorttitle = {Active {Inference} in {Robotics} and {Artificial} {Agents}},
	url = {http://arxiv.org/abs/2112.01871},
	doi = {10.48550/arXiv.2112.01871},
	abstract = {Active inference is a mathematical framework which originated in computational neuroscience as a theory of how the brain implements action, perception and learning. Recently, it has been shown to be a promising approach to the problems of state-estimation and control under uncertainty, as well as a foundation for the construction of goal-driven behaviours in robotics and artificial agents in general. Here, we review the state-of-the-art theory and implementations of active inference for state-estimation, control, planning and learning; describing current achievements with a particular focus on robotics. We showcase relevant experiments that illustrate its potential in terms of adaptation, generalization and robustness. Furthermore, we connect this approach with other frameworks and discuss its expected benefits and challenges: a unified framework with functional biological plausibility using variational Bayesian inference.},
	urldate = {2025-05-14},
	publisher = {arXiv},
	author = {Lanillos, Pablo and Meo, Cristian and Pezzato, Corrado and Meera, Ajith Anil and Baioumy, Mohamed and Ohata, Wataru and Tschantz, Alexander and Millidge, Beren and Wisse, Martijn and Buckley, Christopher L. and Tani, Jun},
	month = dec,
	year = {2021},
	note = {arXiv:2112.01871 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:C\:\\Users\\elias\\Zotero\\storage\\DXY9ICJN\\Lanillos et al. - 2021 - Active Inference in Robotics and Artificial Agents Survey and Challenges.pdf:application/pdf;Snapshot:C\:\\Users\\elias\\Zotero\\storage\\4JUQHTIH\\2112.html:text/html},
}

@misc{danilenka_adaptive_2025,
	title = {Adaptive {Active} {Inference} {Agents} for {Heterogeneous} and {Lifelong} {Federated} {Learning}},
	url = {http://arxiv.org/abs/2410.09099},
	doi = {10.48550/arXiv.2410.09099},
	abstract = {Handling heterogeneity and unpredictability are two core problems in pervasive computing. The challenge is to seamlessly integrate devices with varying computational resources in a dynamic environment to form a cohesive system that can fulfill the needs of all participants. Existing work on adaptive systems typically focuses on optimizing individual variables or low-level Service Level Objectives (SLOs), such as constraining the usage of specific resources. While low-level control mechanisms permit fine-grained control over a system, they introduce considerable complexity, particularly in dynamic environments. To this end, we propose drawing from Active Inference (AIF), a neuroscientific framework for designing adaptive agents. Specifically, we introduce a conceptual agent for heterogeneous pervasive systems that permits setting global systems constraints as high-level SLOs. Instead of manually setting low-level SLOs, the system finds an equilibrium that can adapt to environmental changes. We demonstrate the viability of our AIF agents with an extensive experiment design, using heterogeneous and lifelong federated learning as an application scenario. We conduct our experiments on a physical testbed of devices with different resource types and vendor specifications. The results provide convincing evidence that an AIF agent can adapt a system to environmental changes. In particular, the AIF agent can balance competing SLOs in resource heterogeneous environments to ensure up to 98\% fulfillment rate.},
	urldate = {2025-05-14},
	publisher = {arXiv},
	author = {Danilenka, Anastasiya and Furutanpey, Alireza and Pujol, Victor Casamayor and Sedlak, Boris and Lackinger, Anna and Ganzha, Maria and Paprzycki, Marcin and Dustdar, Schahram},
	month = mar,
	year = {2025},
	note = {arXiv:2410.09099 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\elias\\Zotero\\storage\\AEKHGCS7\\Danilenka et al. - 2025 - Adaptive Active Inference Agents for Heterogeneous and Lifelong Federated Learning.pdf:application/pdf;Snapshot:C\:\\Users\\elias\\Zotero\\storage\\VSGFNBR9\\2410.html:text/html},
}

@misc{sedlak_adaptive_2024,
	title = {Adaptive {Stream} {Processing} on {Edge} {Devices} through {Active} {Inference}},
	url = {http://arxiv.org/abs/2409.17937},
	doi = {10.48550/arXiv.2409.17937},
	abstract = {The current scenario of IoT is witnessing a constant increase on the volume of data, which is generated in constant stream, calling for novel architectural and logical solutions for processing it. Moving the data handling towards the edge of the computing spectrum guarantees better distribution of load and, in principle, lower latency and better privacy. However, managing such a structure is complex, especially when requirements, also referred to Service Level Objectives (SLOs), specified by applications' owners and infrastructure managers need to be ensured. Despite the rich number of proposals of Machine Learning (ML) based management solutions, researchers and practitioners yet struggle to guarantee long-term prediction and control, and accurate troubleshooting. Therefore, we present a novel ML paradigm based on Active Inference (AIF) -- a concept from neuroscience that describes how the brain constantly predicts and evaluates sensory information to decrease long-term surprise. We implement it and evaluate it in a heterogeneous real stream processing use case, where an AIF-based agent continuously optimizes the fulfillment of three SLOs for three autonomous driving services running on multiple devices. The agent used causal knowledge to gradually develop an understanding of how its actions are related to requirements fulfillment, and which configurations to favor. Through this approach, our agent requires up to thirty iterations to converge to the optimal solution, showing the capability of offering accurate results in a short amount of time. Furthermore, thanks to AIF and its causal structures, our method guarantees full transparency on the decision making, making the interpretation of the results and the troubleshooting effortless.},
	urldate = {2025-05-14},
	publisher = {arXiv},
	author = {Sedlak, Boris and Pujol, Victor Casamayor and Morichetta, Andrea and Donta, Praveen Kumar and Dustdar, Schahram},
	month = sep,
	year = {2024},
	note = {arXiv:2409.17937 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\elias\\Zotero\\storage\\7FXQ5V95\\Sedlak et al. - 2024 - Adaptive Stream Processing on Edge Devices through Active Inference.pdf:application/pdf;Snapshot:C\:\\Users\\elias\\Zotero\\storage\\C975ZCIG\\2409.html:text/html},
}

@inproceedings{sedlak_active_2024,
	title = {Active {Inference} on the {Edge}: {A} {Design} {Study}},
	shorttitle = {Active {Inference} on the {Edge}},
	url = {https://ieeexplore.ieee.org/document/10502828},
	doi = {10.1109/PerComWorkshops59983.2024.10502828},
	abstract = {Every year, the amount of data created by Internet of Things (IoT) devices increases; therefore, data processing is carried out by edge devices in close proximity. To ensure Quality of Service (QoS) throughout these operations, systems are supervised and adapted with the help of Machine Learning (ML). However, as long as ML models are not retrained, they fail to capture gradual shifts in the variable distribution, leading to an inaccurate view of the system state and poor inference. In this paper, we present a novel ML paradigm that is constructed upon Active Inference (ACI) – a concept from neuroscience that describes how the brain constantly predicts and evaluates sensory information to decrease long-term surprise. We implemented a use case, in which an ACI-based agent continuously optimized the operation on a smart manufacturing engine according to QoS requirements. The agent used causal knowledge to gradually develop an understanding of how its actions are related to requirements fulfillment, and which configurations to favor. As a result, our agent required 5 cycles to converge to the optimal solution.},
	urldate = {2025-05-14},
	booktitle = {2024 {IEEE} {International} {Conference} on {Pervasive} {Computing} and {Communications} {Workshops} and other {Affiliated} {Events} ({PerCom} {Workshops})},
	author = {Sedlak, Boris and Pujol, Victor Casamayor and Donta, Praveen Kumar and Dustdar, Schahram},
	month = mar,
	year = {2024},
	note = {ISSN: 2766-8576},
	keywords = {Adaptation models, Machine Learning, Quality of service, Active Inference, Conferences, Data models, Edge Intelligence, Markov Blanket, Pervasive computing, Service Level Objectives, Throughput, Training},
	pages = {550--555},
	file = {Full Text PDF:C\:\\Users\\elias\\Zotero\\storage\\846UPI7I\\Sedlak et al. - 2024 - Active Inference on the Edge A Design Study.pdf:application/pdf},
}

@article{sedlak_equilibrium_2024,
	title = {Equilibrium in the {Computing} {Continuum} through {Active} {Inference}},
	volume = {160},
	issn = {0167-739X},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X24002887},
	doi = {10.1016/j.future.2024.05.056},
	abstract = {Computing Continuum (CC) systems are challenged to ensure the intricate requirements of each computational tier. Given the system’s scale, the Service Level Objectives (SLOs), which are expressed as these requirements, must be disaggregated into smaller parts that can be decentralized. We present our framework for collaborative edge intelligence, enabling individual edge devices to (1) develop a causal understanding of how to enforce their SLOs and (2) transfer knowledge to speed up the onboarding of heterogeneous devices. Through collaboration, they (3) increase the scope of SLO fulfillment. We implemented the framework and evaluated a use case in which a CC system is responsible for ensuring Quality of Service (QoS) and Quality of Experience (QoE) during video streaming. Our results showed that edge devices required only ten training rounds to ensure four SLOs; furthermore, the underlying causal structures were also rationally explainable. The addition of new types of devices can be done a posteriori; the framework allowed them to reuse existing models, even though the device type had been unknown. Finally, rebalancing the load within a device cluster allowed individual edge devices to recover their SLO compliance after a network failure from 22\% to 89\%.},
	urldate = {2025-05-14},
	journal = {Future Generation Computer Systems},
	author = {Sedlak, Boris and Pujol, Victor Casamayor and Donta, Praveen Kumar and Dustdar, Schahram},
	month = nov,
	year = {2024},
	keywords = {Active Inference, Computing Continuum, Edge intelligence, Equilibrium, Scalability, Transfer learning},
	pages = {92--108},
	file = {PDF:C\:\\Users\\elias\\Zotero\\storage\\H8HNH39F\\Sedlak et al. - 2024 - Equilibrium in the Computing Continuum through Active Inference.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\elias\\Zotero\\storage\\YFKB8M8I\\S0167739X24002887.html:text/html},
}

@inproceedings{sedlak_markov_2024,
	title = {Markov {Blanket} {Composition} of {SLOs}},
	url = {https://ieeexplore.ieee.org/document/10646438},
	doi = {10.1109/EDGE62653.2024.00025},
	abstract = {Smart environments use composable microservices pipelines to process Internet of Things (IoT) data, where each service is dependent on the outcome of its predecessor. To ensure Quality of Service (QoS), individual services must fulfill Service Level Objectives (SLOs); however, SLO fulfillment is dependent on resources (e.g., processing or storage), which are scarcely available within the Edge. Hence, when distributing services over heterogeneous devices, this raises the question of where to deploy each service to best fulfill both its own SLOs as well as those imposed by dependent services. In this paper, we maximize SLO fulfillment of a pipeline-based application by analyzing these dependencies. To achieve this, services and hosting devices alike are extended with a Markov blanket (MB) - a probabilistic view into their internal processes - which are composed into one overarching model. Given a mutable set of services, hosts, and SLOs, the composed MB allows inferring the optimal assignment between services and edge devices. We evaluated our method for a smart city scenario, which assigned pipelined services (e.g., video processing) under constraints from subsequent services (e.g., consumer latency). The results showed how our method can support infrastructure providers by optimizing SLO fulfillment for arbitrary devices currently available.},
	urldate = {2025-05-14},
	booktitle = {2024 {IEEE} {International} {Conference} on {Edge} {Computing} and {Communications} ({EDGE})},
	author = {Sedlak, Boris and Pujol, Victor Casamayor and Donta, Praveen Kumar and Dustdar, Schahram},
	month = jul,
	year = {2024},
	note = {ISSN: 2767-9918},
	keywords = {Microservice architectures, Quality of service, Edge computing, Markov Blanket, Service Level Objectives, Computing Continuum, Bayesian Networks, Internet of Things, Pipelines, Probabilistic logic, Quality of Service, Smart cities},
	pages = {128--138},
	file = {Full Text PDF:C\:\\Users\\elias\\Zotero\\storage\\AK8QR783\\Sedlak et al. - 2024 - Markov Blanket Composition of SLOs.pdf:application/pdf},
}

@article{pujol_causality_2024,
	title = {On {Causality} in {Distributed} {Continuum} {Systems}},
	volume = {28},
	issn = {1941-0131},
	url = {https://ieeexplore.ieee.org/document/10508275},
	doi = {10.1109/MIC.2023.3344248},
	abstract = {As distributed continuum systems (DCSs) are envisioned, they will have a massive impact on our future society. Hence, it is of utmost importance to ensure that their impact is socially responsible. Equipping these systems with causal models brings features such as explainability, accountability, and auditability, which are needed to provide the right level of trust. Furthermore, by combining causality with graph-based service-level objectives, we can cope with dynamic and complex system requirements while achieving sustainable development of DCSs’ capacities and applications.},
	number = {2},
	urldate = {2025-05-14},
	journal = {IEEE Internet Computing},
	author = {Pujol, Víctor Casamayor and Sedlak, Boris and Donta, Praveen Kumar and Dustdar, Schahram},
	month = mar,
	year = {2024},
	keywords = {Complex systems, Distributed computing, Service-oriented architecture, Social factors, Sustainable development, Trusted computing},
	pages = {57--64},
	file = {Full Text PDF:C\:\\Users\\elias\\Zotero\\storage\\MH86T5A5\\Pujol et al. - 2024 - On Causality in Distributed Continuum Systems.pdf:application/pdf},
}

@inproceedings{sedlak_diffusing_2024,
	title = {Diffusing {High}-level {SLO} in {Microservice} {Pipelines}},
	url = {https://ieeexplore.ieee.org/document/10685329},
	doi = {10.1109/SOSE62363.2024.00008},
	abstract = {Complex interactions within microservice architectures obfuscate the implications of individual services to high-level requirements. This becomes even more grave for multi-tenant and multi-vendor scenarios, like Edge computing, where different stakeholders might specify opposing Service Level Objectives (SLOs), e.g., minimizing both energy consumption and response time. To avoid contradictions within SLOs and to infer how SLOs can be fulfilled, this paper presents a methodology that diffuses high-level SLOs into multiple lower levels of SLOs and parameter assignments. Thus, it becomes clear how individual sub-processes contribute to high-level SLOs, and how these must be configured to foster their fulfillment. We evaluated our methodology for several microservice pipelines, where the challenge is to ensure multiple high-level SLOs (e.g., customer satisfaction) by finding and constraining all influential factors. The results show that by inferring multiple layers of lower-level constraints, we can fulfill high-level SLOs up to 100\%. Notably, we could extract that the restrictiveness of low-level SLOs and the occurrence of conflicts have a severe impact on SLO fulfillment.},
	urldate = {2025-05-14},
	booktitle = {2024 {IEEE} {International} {Conference} on {Service}-{Oriented} {System} {Engineering} ({SOSE})},
	author = {Sedlak, Boris and Pujol, Víctor Casamayor and Donta, Praveen Kumar and Dustdar, Schahram},
	month = jul,
	year = {2024},
	note = {ISSN: 2642-6587},
	keywords = {Time factors, Microservice architectures, Service Level Objectives, Pipelines, Computer architecture, Customer satisfaction, Edge Computing, Energy consumption, Intelligent Systems, Microservices, Requirements Assurance, Service-oriented systems engineering},
	pages = {11--19},
	file = {Full Text PDF:C\:\\Users\\elias\\Zotero\\storage\\9YGLFJ32\\Sedlak et al. - 2024 - Diffusing High-level SLO in Microservice Pipelines.pdf:application/pdf},
}

@misc{sedlak_towards_2025,
	title = {Towards {Multi}-dimensional {Elasticity} for {Pervasive} {Stream} {Processing} {Services}},
	url = {http://arxiv.org/abs/2503.04193},
	doi = {10.48550/arXiv.2503.04193},
	abstract = {This paper proposes a hierarchical solution to scale streaming services across quality and resource dimensions. Modern scenarios, like smart cities, heavily rely on the continuous processing of IoT data to provide real-time services and meet application targets (Service Level Objectives -- SLOs). While the tendency is to process data at nearby Edge devices, this creates a bottleneck because resources can only be provisioned up to a limited capacity. To improve elasticity in Edge environments, we propose to scale services in multiple dimensions -- either resources or, alternatively, the service quality. We rely on a two-layer architecture where (1) local, service-specific agents ensure SLO fulfillment through multi-dimensional elasticity strategies; if no more resources can be allocated, (2) a higher-level agent optimizes global SLO fulfillment by swapping resources. The experimental results show promising outcomes, outperforming regular vertical autoscalers, when operating under tight resource constraints.},
	urldate = {2025-05-14},
	publisher = {arXiv},
	author = {Sedlak, Boris and Morichetta, Andrea and Raith, Philipp and Pujol, Víctor Casamayor and Dustdar, Schahram},
	month = mar,
	year = {2025},
	note = {arXiv:2503.04193 [cs]},
	keywords = {Computer Science - Performance},
	file = {Preprint PDF:C\:\\Users\\elias\\Zotero\\storage\\T5UH6FSQ\\Sedlak et al. - 2025 - Towards Multi-dimensional Elasticity for Pervasive Stream Processing Services.pdf:application/pdf;Snapshot:C\:\\Users\\elias\\Zotero\\storage\\SEZKS5K2\\2503.html:text/html},
}

@misc{lapkovskis_benchmarking_2025,
	title = {Benchmarking {Dynamic} {SLO} {Compliance} in {Distributed} {Computing} {Continuum} {Systems}},
	url = {http://arxiv.org/abs/2503.03274},
	doi = {10.48550/arXiv.2503.03274},
	abstract = {Ensuring Service Level Objectives (SLOs) in large-scale architectures, such as Distributed Computing Continuum Systems (DCCS), is challenging due to their heterogeneous nature and varying service requirements across different devices and applications. Additionally, unpredictable workloads and resource limitations lead to fluctuating performance and violated SLOs. To improve SLO compliance in DCCS, one possibility is to apply machine learning; however, the design choices are often left to the developer. To that extent, we provide a benchmark of Active Inference -- an emerging method from neuroscience -- against three established reinforcement learning algorithms (Deep Q-Network, Advantage Actor-Critic, and Proximal Policy Optimization). We consider a realistic DCCS use case: an edge device running a video conferencing application alongside a WebSocket server streaming videos. Using one of the respective algorithms, we continuously monitor key performance metrics, such as latency and bandwidth usage, to dynamically adjust parameters -- including the number of streams, frame rate, and resolution -- to optimize service quality and user experience. To test algorithms' adaptability to constant system changes, we simulate dynamically changing SLOs and both instant and gradual data-shift scenarios, such as network bandwidth limitations and fluctuating device thermal states. Although the evaluated algorithms all showed advantages and limitations, our findings demonstrate that Active Inference is a promising approach for ensuring SLO compliance in DCCS, offering lower memory usage, stable CPU utilization, and fast convergence.},
	urldate = {2025-05-14},
	publisher = {arXiv},
	author = {Lapkovskis, Alfreds and Sedlak, Boris and Magnússon, Sindri and Dustdar, Schahram and Donta, Praveen Kumar},
	month = mar,
	year = {2025},
	note = {arXiv:2503.03274 [cs]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Networking and Internet Architecture, Computer Science - Performance},
	file = {Preprint PDF:C\:\\Users\\elias\\Zotero\\storage\\ECEZ9HAH\\Lapkovskis et al. - 2025 - Benchmarking Dynamic SLO Compliance in Distributed Computing Continuum Systems.pdf:application/pdf;Snapshot:C\:\\Users\\elias\\Zotero\\storage\\ZYKYGITY\\2503.html:text/html},
}

@article{dean_mapreduce_2008,
	title = {{MapReduce}: simplified data processing on large clusters},
	volume = {51},
	issn = {0001-0782},
	shorttitle = {{MapReduce}},
	url = {https://dl.acm.org/doi/10.1145/1327452.1327492},
	doi = {10.1145/1327452.1327492},
	abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
	number = {1},
	urldate = {2025-05-14},
	journal = {Commun. ACM},
	author = {Dean, Jeffrey and Ghemawat, Sanjay},
	month = jan,
	year = {2008},
	pages = {107--113},
	file = {Full Text PDF:C\:\\Users\\elias\\Zotero\\storage\\2P7BVNFM\\Dean and Ghemawat - 2008 - MapReduce simplified data processing on large clusters.pdf:application/pdf},
}

@inproceedings{sedlak_slo-aware_2025,
	address = {Singapore},
	title = {{SLO}-{Aware} {Task} {Offloading} {Within} {Collaborative} {Vehicle} {Platoons}},
	isbn = {978-981-96-0808-9},
	doi = {10.1007/978-981-96-0808-9_6},
	abstract = {In the context of autonomous vehicles (AVs), offloading is essential for guaranteeing the execution of perception tasks, e.g., mobile mapping or object detection. While existing work on offloading focused extensively on minimizing inter-vehicle networking latency, vehicle platoons (e.g., heavy-duty transport) present numerous other objectives, such as energy efficiency or data quality. To optimize these Service Level Objectives (SLOs) during operation, this work presents a purely Vehicle-to-Vehicle approach (V2V) for collaborative services offloading within a vehicle platoon. By training and using a Bayesian Network (BN), services can proactively decide to offload whenever this promises to improve platoon-wide SLO fulfillment; therefore, vehicles estimate how both sides would be impacted by offloading a service. In particular, this considers resource heterogeneity within the platoon to avoid overloading more restricted devices. We evaluate our approach in a physical setup, where vehicles in a platoon continuously (i.e., every 500 ms) interpret the SLOs of three perception services. Our probabilistic, predictive method shows promising results in handling large AV platoons; within seconds, it detects and resolves SLO violations through offloading.},
	language = {en},
	booktitle = {Service-{Oriented} {Computing}},
	publisher = {Springer Nature},
	author = {Sedlak, Boris and Morichetta, Andrea and Wang, Yuhao and Fei, Yang and Wang, Liang and Dustdar, Schahram and Qu, Xiaobo},
	editor = {Gaaloul, Walid and Sheng, Michael and Yu, Qi and Yangui, Sami},
	year = {2025},
	keywords = {Service Level Objectives, Bayesian Networks, Edge Computing, Microservices, Intelligent Transportation, Offloading},
	pages = {72--86},
	file = {Full Text PDF:C\:\\Users\\elias\\Zotero\\storage\\FPY873DS\\Sedlak et al. - 2025 - SLO-Aware Task Offloading Within Collaborative Vehicle Platoons.pdf:application/pdf},
}

@article{sajid_active_2021,
	title = {Active {Inference}: {Demystified} and {Compared}},
	volume = {33},
	issn = {0899-7667},
	shorttitle = {Active {Inference}},
	url = {https://doi.org/10.1162/neco_a_01357},
	doi = {10.1162/neco_a_01357},
	abstract = {Active inference is a first principle account of how autonomous agents operate in dynamic, nonstationary environments. This problem is also considered in reinforcement learning, but limited work exists on comparing the two approaches on the same discrete-state environments. In this letter, we provide (1) an accessible overview of the discrete-state formulation of active inference, highlighting natural behaviors in active inference that are generally engineered in reinforcement learning, and (2) an explicit discrete-state comparison between active inference and reinforcement learning on an OpenAI gym baseline. We begin by providing a condensed overview of the active inference literature, in particular viewing the various natural behaviors of active inference agents through the lens of reinforcement learning. We show that by operating in a pure belief-based setting, active inference agents can carry out epistemic exploration—and account for uncertainty about their
environment—in a Bayes-optimal fashion. Furthermore, we show that the reliance on an explicit reward signal in reinforcement learning is removed in active inference, where reward can simply be treated as another observation we have a preference over; even in the total absence of rewards, agent behaviors are learned through preference learning. We make these properties explicit by showing two scenarios in which active inference agents can infer behaviors in reward-free environments compared to both Q-learning and Bayesian model-based reinforcement learning agents and by placing zero prior preferences over rewards and learning the prior preferences over the observations corresponding to reward. We conclude by noting that this formalism can be applied to more complex settings (e.g., robotic arm movement, Atari games) if appropriate generative models can be formulated. In short, we aim to demystify the behavior of active inference agents by presenting an accessible discrete
state-space and time formulation and demonstrate these behaviors in a OpenAI gym environment, alongside reinforcement learning agents.},
	number = {3},
	urldate = {2025-05-15},
	journal = {Neural Computation},
	author = {Sajid, Noor and Ball, Philip J. and Parr, Thomas and Friston, Karl J.},
	month = mar,
	year = {2021},
	pages = {674--712},
	file = {Full Text:C\:\\Users\\elias\\Zotero\\storage\\9JKRTHXM\\Sajid et al. - 2021 - Active Inference Demystified and Compared.pdf:application/pdf;Snapshot:C\:\\Users\\elias\\Zotero\\storage\\E8ACPKA3\\Active-Inference-Demystified-and-Compared.html:text/html},
}

@misc{jocher_ultralytics_2023,
	title = {Ultralytics {YOLO}},
	url = {https://github.com/ultralytics/ultralytics},
	author = {Jocher, Glenn and Qiu, Jing and Chaurasia, Ayush},
	month = jan,
	year = {2023},
}

@misc{khanam_yolov11_2024,
	title = {{YOLOv11}: {An} {Overview} of the {Key} {Architectural} {Enhancements}},
	shorttitle = {{YOLOv11}},
	url = {http://arxiv.org/abs/2410.17725},
	doi = {10.48550/arXiv.2410.17725},
	abstract = {This study presents an architectural analysis of YOLOv11, the latest iteration in the YOLO (You Only Look Once) series of object detection models. We examine the models architectural innovations, including the introduction of the C3k2 (Cross Stage Partial with kernel size 2) block, SPPF (Spatial Pyramid Pooling - Fast), and C2PSA (Convolutional block with Parallel Spatial Attention) components, which contribute in improving the models performance in several ways such as enhanced feature extraction. The paper explores YOLOv11's expanded capabilities across various computer vision tasks, including object detection, instance segmentation, pose estimation, and oriented object detection (OBB). We review the model's performance improvements in terms of mean Average Precision (mAP) and computational efficiency compared to its predecessors, with a focus on the trade-off between parameter count and accuracy. Additionally, the study discusses YOLOv11's versatility across different model sizes, from nano to extra-large, catering to diverse application needs from edge devices to high-performance computing environments. Our research provides insights into YOLOv11's position within the broader landscape of object detection and its potential impact on real-time computer vision applications.},
	urldate = {2025-05-15},
	publisher = {arXiv},
	author = {Khanam, Rahima and Hussain, Muhammad},
	month = oct,
	year = {2024},
	note = {arXiv:2410.17725 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\elias\\Zotero\\storage\\KAQN6FMH\\Khanam and Hussain - 2024 - YOLOv11 An Overview of the Key Architectural Enhancements.pdf:application/pdf;Snapshot:C\:\\Users\\elias\\Zotero\\storage\\M7GVGTEL\\2410.html:text/html},
}

@misc{alif_yolov11_2024,
	title = {{YOLOv11} for {Vehicle} {Detection}: {Advancements}, {Performance}, and {Applications} in {Intelligent} {Transportation} {Systems}},
	shorttitle = {{YOLOv11} for {Vehicle} {Detection}},
	url = {http://arxiv.org/abs/2410.22898},
	doi = {10.48550/arXiv.2410.22898},
	abstract = {Accurate vehicle detection is essential for the development of intelligent transportation systems, autonomous driving, and traffic monitoring. This paper presents a detailed analysis of YOLO11, the latest advancement in the YOLO series of deep learning models, focusing exclusively on vehicle detection tasks. Building upon the success of its predecessors, YOLO11 introduces architectural improvements designed to enhance detection speed, accuracy, and robustness in complex environments. Using a comprehensive dataset comprising multiple vehicle types-cars, trucks, buses, motorcycles, and bicycles we evaluate YOLO11's performance using metrics such as precision, recall, F1 score, and mean average precision (mAP). Our findings demonstrate that YOLO11 surpasses previous versions (YOLOv8 and YOLOv10) in detecting smaller and more occluded vehicles while maintaining a competitive inference time, making it well-suited for real-time applications. Comparative analysis shows significant improvements in the detection of complex vehicle geometries, further contributing to the development of efficient and scalable vehicle detection systems. This research highlights YOLO11's potential to enhance autonomous vehicle performance and traffic monitoring systems, offering insights for future developments in the field.},
	urldate = {2025-05-15},
	publisher = {arXiv},
	author = {Alif, Mujadded Al Rabbani},
	month = oct,
	year = {2024},
	note = {arXiv:2410.22898 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\elias\\Zotero\\storage\\W362QVZF\\Alif - 2024 - YOLOv11 for Vehicle Detection Advancements, Performance, and Applications in Intelligent Transporta.pdf:application/pdf;Snapshot:C\:\\Users\\elias\\Zotero\\storage\\RWKXHHDE\\2410.html:text/html},
}

@inproceedings{varghese_yolov8_2024,
	title = {{YOLOv8}: {A} {Novel} {Object} {Detection} {Algorithm} with {Enhanced} {Performance} and {Robustness}},
	shorttitle = {{YOLOv8}},
	url = {https://ieeexplore.ieee.org/abstract/document/10533619},
	doi = {10.1109/ADICS58448.2024.10533619},
	abstract = {In recent years, the You Only Look Once (YOLO) series of object detection algorithms have garnered significant attention for their speed and accuracy in real-time applications. This paper presents YOLOv8, a novel object detection algorithm that builds upon the advancements of previous iterations, aiming to further enhance performance and robustness. Inspired by the evolution of YOLO architectures from YOLOv1 to YOLOv7, as well as insights from comparative analyses of models like YOLOv5 and YOLOv6, YOLOv8 incorporates key innovations to achieve optimal speed and accuracy. Leveraging attention mechanisms and dynamic convolution, YOLOv8 introduces improvements specifically tailored for small object detection, addressing challenges highlighted in YOLOv7. Additionally, the integration of voice recognition techniques enhances the algorithm's capabilities for video-based object detection, as demonstrated in YOLOv7. The proposed algorithm undergoes rigorous evaluation against state-of-the-art benchmarks, showcasing superior performance in terms of both detection accuracy and computational efficiency. Experimental results on various datasets confirm the effectiveness of YOLOv8 across diverse scenarios, further validating its suitability for real-world applications. This paper contributes to the ongoing advancements in object detection research by presenting YOLOv8 as a versatile and high-performing algorithm, poised to address the evolving needs of computer vision systems.},
	urldate = {2025-05-15},
	booktitle = {2024 {International} {Conference} on {Advances} in {Data} {Engineering} and {Intelligent} {Computing} {Systems} ({ADICS})},
	author = {Varghese, Rejin and M., Sambath},
	month = apr,
	year = {2024},
	keywords = {Heuristic algorithms, Benchmark testing, Computational Efficiency, Computer vision, Computer Vision Systems, Object Detection, Performance Enhancement, Performance evaluation, Robustness, Speech recognition, Technological innovation, YOLO, YOLOv8},
	pages = {1--6},
	file = {Full Text PDF:C\:\\Users\\elias\\Zotero\\storage\\DQFV6X3L\\Varghese and M. - 2024 - YOLOv8 A Novel Object Detection Algorithm with Enhanced Performance and Robustness.pdf:application/pdf},
}

@misc{noauthor_opencvopencv_nodate,
	title = {opencv/opencv at 4.11.0},
	url = {https://github.com/opencv/opencv/tree/4.11.0},
	urldate = {2025-05-15},
	file = {opencv/opencv at 4.11.0:C\:\\Users\\elias\\Zotero\\storage\\6TZ962K8\\4.11.html:text/html},
}

@misc{noauthor_zeromqpyzmq_nodate,
	title = {zeromq/pyzmq at v26.4.0},
	url = {https://github.com/zeromq/pyzmq},
	abstract = {PyZMQ:  Python bindings for zeromq. Contribute to zeromq/pyzmq development by creating an account on GitHub.},
	language = {en},
	urldate = {2025-05-15},
	journal = {GitHub},
	file = {Snapshot:C\:\\Users\\elias\\Zotero\\storage\\KE4LVYXY\\v26.4.html:text/html},
}

@misc{noauthor_msgpackmsgpack-python_nodate,
	title = {msgpack/msgpack-python at v1.1.0},
	url = {https://github.com/msgpack/msgpack-python/tree/v1.1.0},
	urldate = {2025-05-15},
	file = {msgpack/msgpack-python at v1.1.0:C\:\\Users\\elias\\Zotero\\storage\\5ZWRYXMR\\v1.1.html:text/html},
}

@article{harris_array_2020,
	title = {Array programming with {NumPy}},
	volume = {585},
	doi = {10.1038/s41586-020-2649-2},
	journal = {Nature},
	author = {Harris, Charles R. and Millman, K. Jarrod and van der Walt, Stéfan J and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and van Kerkwijk, Marten H. and Brett, Matthew and Haldane, Allan and Fernández del Río, Jaime and Wiebe, Mark and Peterson, Pearu and Gérard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
	year = {2020},
	pages = {357--362},
}

@misc{brockman_openai_2016,
	title = {{OpenAI} {Gym}},
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	year = {2016},
	note = {\_eprint: arXiv:1606.01540},
}

@article{dautov_stream_2022,
	title = {Stream {Processing} on {Clustered} {Edge} {Devices}},
	volume = {10},
	issn = {2168-7161},
	url = {https://ieeexplore.ieee.org/abstract/document/9051851},
	doi = {10.1109/TCC.2020.2983402},
	abstract = {The Internet of Things continuously generates avalanches of raw sensor data to be transferred to the Cloud for processing and storage. Due to network latency and limited bandwidth, this vertical offloading model, however, fails to meet requirements of time-critical data-intensive applications which must act upon generated data with minimum time delays. To address such a limitation, this article proposes a novel distributed architecture enabling stream data processing at the edge of the network, broadening the principle of enabling processing closer to data sources adopted by Fog and Edge Computing. Specifically, this architecture extends the Apache NiFi stream processing middleware with support for run-time clustering of heterogeneous edge devices, such that computational tasks can be horizontally offloaded to peer devices and executed in parallel. As opposed to vertical offloading on the Cloud, the proposed solution does not suffer from increased network latency and is thus able to offer 5-25 times faster response time, as demonstrated by the experiments on a run-time license plate recognition system.},
	number = {2},
	urldate = {2025-05-16},
	journal = {IEEE Transactions on Cloud Computing},
	author = {Dautov, Rustem and Distefano, Salvatore},
	month = apr,
	year = {2022},
	keywords = {Cloud computing, Edge computing, Internet of Things, Computer architecture, apache NiFi, big data, Data processing, edge computing, horizontal and vertical offloading, license plate recognition, License plate recognition, Middleware, stream processing, Task analysis},
	pages = {885--898},
	file = {Full Text PDF:C\:\\Users\\elias\\Zotero\\storage\\QJVJHEMP\\Dautov and Distefano - 2022 - Stream Processing on Clustered Edge Devices.pdf:application/pdf},
}

@misc{noauthor_apache_nodate,
	title = {Apache {Storm}},
	url = {https://storm.apache.org/},
	urldate = {2025-05-16},
	file = {Apache Storm:C\:\\Users\\elias\\Zotero\\storage\\PF94J4DR\\storm.apache.org.html:text/html},
}

@inproceedings{casamayor_pujol_deepslos_2024,
	address = {New York, NY, USA},
	series = {{ApPLIED}'24},
	title = {{DeepSLOs} for the {Computing} {Continuum}},
	isbn = {979-8-4007-0670-7},
	shorttitle = {Invited {Paper}},
	url = {https://dl.acm.org/doi/10.1145/3663338.3663681},
	doi = {10.1145/3663338.3663681},
	abstract = {The advent of the computing continuum, i.e., the blending of all existing computational tiers, calls for novel techniques and methods that consider its complex dynamics. This work presents the DeepSLO as a novel design paradigm to define and structure Service Level Objectives (SLOs) for distributed computing continuum systems. Hence, when multiple stakeholders are involved, the DeepSLO allows them to plan the overarching behaviors of the system. Further, the techniques employed (Bayesian networks, Markov blanket, Active inference) provide autonomy and decentralization to each SLO while the DeepSLO hierarchy remains to account for objectives dependencies. Finally, DeepSLOs are represented graphically, as well as individual SLOs enabling a human interpretation of the system performance.},
	urldate = {2025-05-17},
	booktitle = {Proceedings of the 2024 {Workshop} on {Advanced} {Tools}, {Programming} {Languages}, and {PLatforms} for {Implementing} and {Evaluating} algorithms for {Distributed} systems},
	publisher = {Association for Computing Machinery},
	author = {Casamayor Pujol, Victor and Sedlak, Boris and Xu, Yanwei and Donta, Praveen Kumar and Dustdar, Schahram},
	month = jun,
	year = {2024},
	pages = {1--10},
	file = {Full Text PDF:C\:\\Users\\elias\\Zotero\\storage\\WICKJSAF\\Casamayor Pujol et al. - 2024 - Invited Paper DeepSLOs for the Computing Continuum.pdf:application/pdf},
}

@misc{noauthor_apache_nodate-1,
	title = {Apache {Flink}® — {Stateful} {Computations} over {Data} {Streams}},
	url = {https://flink.apache.org/},
	abstract = {Recent Flink blogs Apache Flink CDC 3.4.0 Release Announcement May 16, 2025 - Yanquan Lv. The Apache Flink Community is excited to announce the release of Flink CDC 3.4.0! This release introduces a new pipeline Connector for Apache Iceberg, and provides support for batch execution mode, … Continue reading Introducing the Externalized Kudu Connector April 30, 2025 - Ferenc Csaky. We are pleased to announce the revival of a connector that makes it possible for Flink to interact with Apache Kudu.},
	language = {en},
	urldate = {2025-05-24},
	file = {Snapshot:C\:\\Users\\elias\\Zotero\\storage\\DUV7LWDS\\flink.apache.org.html:text/html},
}

@article{carbone_apache_2015,
	title = {Apache {Flink}™: {Stream} and {Batch} {Processing} in a {Single} {Engine}},
	abstract = {Apache Flink1 is an open-source system for processing streaming and batch data. Flink is built on the philosophy that many classes of data processing applications, including real-time analytics, continuous data pipelines, historic data processing (batch), and iterative algorithms (machine learning, graph analysis) can be expressed and executed as pipelined fault-tolerant dataﬂows. In this paper, we present Flink’s architecture and expand on how a (seemingly diverse) set of use cases can be uniﬁed under a single execution model.},
	language = {en},
	author = {Carbone, Paris and Katsifodimos, Asterios and Ewen, Stephan and Markl, Volker and Haridi, Seif and Tzoumas, Kostas},
	year = {2015},
	file = {PDF:C\:\\Users\\elias\\Zotero\\storage\\ZZCS5BV3\\Carbone et al. - Apache Flink™ Stream and Batch Processing in a Single Engine.pdf:application/pdf},
}

@article{raffin_stable-baselines3_2021,
	title = {Stable-{Baselines3}: {Reliable} {Reinforcement} {Learning} {Implementations}},
	volume = {22},
	issn = {1533-7928},
	shorttitle = {Stable-{Baselines3}},
	url = {http://jmlr.org/papers/v22/20-1364.html},
	abstract = {Stable-Baselines3 provides open-source implementations of deep reinforcement learning (RL) algorithms in Python. The implementations have been benchmarked against reference codebases, and automated unit tests cover 95\% of the code. The algorithms follow a consistent interface and are accompanied by extensive documentation, making it simple to train and compare different RL algorithms. Our documentation, examples, and source-code are available at https://github.com/DLR-RM/stable-baselines3.},
	number = {268},
	urldate = {2025-05-30},
	journal = {Journal of Machine Learning Research},
	author = {Raffin, Antonin and Hill, Ashley and Gleave, Adam and Kanervisto, Anssi and Ernestus, Maximilian and Dormann, Noah},
	year = {2021},
	pages = {1--8},
	file = {Full Text PDF:C\:\\Users\\elias\\Zotero\\storage\\9IQHDXZW\\Raffin et al. - 2021 - Stable-Baselines3 Reliable Reinforcement Learning Implementations.pdf:application/pdf;Source Code:C\:\\Users\\elias\\Zotero\\storage\\BNT9S6XD\\stable-baselines3.html:text/html},
}

@article{nastic_sloc_2020,
	title = {{SLOC}: {Service} {Level} {Objectives} for {Next} {Generation} {Cloud} {Computing}},
	volume = {24},
	issn = {1941-0131},
	shorttitle = {{SLOC}},
	url = {https://ieeexplore.ieee.org/abstract/document/9146966},
	doi = {10.1109/MIC.2020.2987739},
	abstract = {Since the emergence of cloud computing service level objectives (SLOs) and service level agreements (SLAs) have put themselves forward as one of the key enablers for cloud's on-demand, pay-as-you-go service consumption model. To date, the vast majority of cloud platforms provide support for SLAs only in terms of statically predefined SLOs, e.g., service availability, and low-level resource capacity guarantees, e.g., CPU usage. Unfortunately, there is only limited support to clearly map workload performance requirements to the resource capacity guarantees. In this article, we introduce SLOC— a novel elasticity framework, which promotes a novel performance-driven, SLO-native approach to cloud computing. We outline the main research challenges, vision, and approach of our SLOC framework toward the SLO-native paradigm in next generation cloud computing.},
	number = {3},
	urldate = {2025-06-01},
	journal = {IEEE Internet Computing},
	author = {Nastic, Stefan and Morichetta, Andrea and Pusztai, Thomas and Dustdar, Schahram and Ding, Xiaoning and Vij, Deepak and Xiong, Ying},
	month = may,
	year = {2020},
	keywords = {Cloud computing, Contracts, Measurement, Quality of service, Resource management, Service level agreements},
	pages = {39--50},
	file = {Full Text PDF:C\:\\Users\\elias\\Zotero\\storage\\5CIZJWRU\\Nastic et al. - 2020 - SLOC Service Level Objectives for Next Generation Cloud Computing.pdf:application/pdf},
}

@article{deng_edge_2020,
	title = {Edge {Intelligence}: {The} {Confluence} of {Edge} {Computing} and {Artificial} {Intelligence}},
	volume = {7},
	issn = {2327-4662},
	shorttitle = {Edge {Intelligence}},
	url = {https://ieeexplore.ieee.org/abstract/document/9052677},
	doi = {10.1109/JIOT.2020.2984887},
	abstract = {Along with the rapid developments in communication technologies and the surge in the use of mobile devices, a brand-new computation paradigm, edge computing, is surging in popularity. Meanwhile, the artificial intelligence (AI) applications are thriving with the breakthroughs in deep learning and the many improvements in hardware architectures. Billions of data bytes, generated at the network edge, put massive demands on data processing and structural optimization. Thus, there exists a strong demand to integrate edge computing and AI, which gives birth to edge intelligence. In this article, we divide edge intelligence into AI for edge (intelligence-enabled edge computing) and AI on edge (artificial intelligence on edge). The former focuses on providing more optimal solutions to key problems in edge computing with the help of popular and effective AI technologies while the latter studies how to carry out the entire process of building AI models, i.e., model training and inference, on the edge. This article provides insights into this new interdisciplinary field from a broader perspective. It discusses the core concepts and the research roadmap, which should provide the necessary background for potential future research initiatives in edge intelligence.},
	number = {8},
	urldate = {2025-06-01},
	journal = {IEEE Internet of Things Journal},
	author = {Deng, Shuiguang and Zhao, Hailiang and Fang, Weijia and Yin, Jianwei and Dustdar, Schahram and Zomaya, Albert Y.},
	month = aug,
	year = {2020},
	keywords = {Cloud computing, Computation offloading, Computational modeling, Computer architecture, Deep learning, edge computing, Edge computing, edge intelligence, Federated learning, Internet of Things, wireless networking (WN)},
	pages = {7457--7469},
	file = {Full Text PDF:C\:\\Users\\elias\\Zotero\\storage\\JGRET4SR\\Deng et al. - 2020 - Edge Intelligence The Confluence of Edge Computing and Artificial Intelligence.pdf:application/pdf},
}
